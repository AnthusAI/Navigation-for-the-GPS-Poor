{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Stereo Visual Odometry - A Practical Demo\n",
    "\n",
    "This notebook is the hands-on companion to the [Chapter 2 article](index.md). Here, we'll upgrade our visual odometry system from Chapter 1 by adding a second camera to solve the **scale problem**.\n",
    "\n",
    "**Our goal**: To use two cameras (stereo vision) to calculate real-world depth and build a trajectory that has the correct scale in meters, without needing GPS or any external reference.\n",
    "\n",
    "We will follow these key steps:\n",
    "1.  **Setup and Data Loading**: Load stereo image pairs from both cameras.\n",
    "2.  **Stereo Matching**: Match features between left and right images to find disparity.\n",
    "3.  **Triangulation**: Calculate 3D positions of features using stereo geometry.\n",
    "4.  **Temporal Tracking**: Track 3D points across time to estimate camera motion.\n",
    "5.  **Pose Estimation with PnP**: Use 3D-to-2D correspondences to recover scaled motion.\n",
    "6.  **Evaluation**: Compare our stereo VO result against Chapter 1's monocular VO and ground truth.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports and Setup ---\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "# This allows us to import our custom modules from the `src` directory\n",
    "# Assumes the notebook is in `chapters/2/`\n",
    "project_root = Path().resolve().parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "print(f\"Project root added to path: {project_root}\")\n",
    "\n",
    "# --- Our Project Modules ---\n",
    "from src.datasets import KITTIDatasetFetcher\n",
    "from src.feature_matching import FeatureMatcher\n",
    "from src.pose_estimation import PoseEstimator\n",
    "from src.visual_odometry import SimpleVisualOdometry, StereoVisualOdometry, run_vo_pipeline, run_stereo_vo_pipeline\n",
    "from src.utils import load_image, plot_trajectory, compute_trajectory_error, plot_top_down_trajectory\n",
    "\n",
    "# --- Matplotlib Setup ---\n",
    "# Set a professional and readable style for our plots\n",
    "plt.style.use('seaborn-v0_8-pastel')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "print(\"âœ… Setup complete. All modules imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download and Load the Stereo Dataset\n",
    "\n",
    "We'll use the same KITTI Visual Odometry dataset from Chapter 1, but this time we'll load **both** the left and right camera images. The KITTI dataset was recorded with a stereo camera rig, where two cameras are mounted side-by-side with a known distance between them (the **baseline**).\n",
    "\n",
    "This baseline distance is critical: it's what allows us to calculate real-world depth using triangulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "SEQUENCE_ID = \"00\"\n",
    "DATA_DIR = project_root / \"data\"\n",
    "\n",
    "# --- Download ---\n",
    "print(\"Setting up dataset...\")\n",
    "# This will automatically download and unzip the data if it's not already present\n",
    "kitti_fetcher = KITTIDatasetFetcher(data_dir=DATA_DIR)\n",
    "kitti_fetcher.download_sequence(SEQUENCE_ID)\n",
    "print(f\"âœ… Sequence '{SEQUENCE_ID}' is ready.\")\n",
    "\n",
    "# --- Load ---\n",
    "print(\"\\nLoading dataset components...\")\n",
    "# Load camera calibration parameters for BOTH cameras\n",
    "calibration = kitti_fetcher.load_calibration(SEQUENCE_ID)\n",
    "K_left = calibration['P0'][:, :3]   # Left camera intrinsics\n",
    "K_right = calibration['P1'][:, :3]  # Right camera intrinsics\n",
    "\n",
    "# Calculate the baseline from the projection matrices\n",
    "# The baseline is encoded in the translation component of P1\n",
    "baseline = abs(calibration['P1'][0, 3] / calibration['P1'][0, 0])\n",
    "\n",
    "print(f\"Left camera intrinsics (K_left):\\n{K_left}\")\n",
    "print(f\"\\nRight camera intrinsics (K_right):\\n{K_right}\")\n",
    "print(f\"\\nStereo baseline: {baseline:.3f} meters\")\n",
    "\n",
    "# Load the ground truth poses\n",
    "ground_truth_poses = kitti_fetcher.load_poses(SEQUENCE_ID)\n",
    "\n",
    "# Get image paths for BOTH cameras\n",
    "left_image_paths = kitti_fetcher.get_image_paths(SEQUENCE_ID, camera=\"image_0\")\n",
    "right_image_paths = kitti_fetcher.get_image_paths(SEQUENCE_ID, camera=\"image_1\")\n",
    "\n",
    "print(f\"\\nâœ… Loaded {len(left_image_paths)} stereo image pairs\")\n",
    "print(f\"âœ… Loaded {len(ground_truth_poses)} ground truth poses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Visualize a Stereo Pair\n",
    "\n",
    "Let's look at an example of a stereo image pair. Notice how the same scene is captured from two slightly different viewpoints. Objects closer to the camera will appear to \"shift\" more between the two views than distant objects. This shift is called **disparity**, and it's directly related to depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display a stereo pair\n",
    "frame_idx = 100\n",
    "img_left = load_image(left_image_paths[frame_idx])\n",
    "img_right = load_image(right_image_paths[frame_idx])\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "axes[0].imshow(img_left, cmap='gray')\n",
    "axes[0].set_title(f'Left Camera - Frame {frame_idx}', fontsize=14)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(img_right, cmap='gray')\n",
    "axes[1].set_title(f'Right Camera - Frame {frame_idx}', fontsize=14)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ‘€ Notice: The two images show the same scene from slightly different angles.\")\n",
    "print(\"   Objects closer to the camera (like the road markings) shift more than distant objects (like the horizon).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Stereo Matching and Triangulation\n",
    "\n",
    "Now we'll demonstrate the core principle of stereo vision: matching features between the left and right images, then using the **disparity** to calculate the 3D position.\n",
    "\n",
    "The key equation is:\n",
    "$$\n",
    "Z = \\frac{\\text{baseline} \\times f}{\\text{disparity}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `Z` is the depth (distance from the camera)\n",
    "- `f` is the focal length\n",
    "- `disparity` is the horizontal pixel shift between the left and right images\n",
    "\n",
    "Once we have `Z`, we can calculate the full 3D position `(X, Y, Z)` using the camera intrinsics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and match features in a stereo pair\n",
    "matcher = FeatureMatcher(detector_type='ORB', max_features=2000)\n",
    "\n",
    "# Detect features in both images\n",
    "kp_left, desc_left = matcher.detect_features(img_left)\n",
    "kp_right, desc_right = matcher.detect_features(img_right)\n",
    "\n",
    "# Match features between left and right\n",
    "stereo_matches = matcher.match_features(desc_left, desc_right)\n",
    "\n",
    "print(f\"Found {len(stereo_matches)} stereo matches\")\n",
    "\n",
    "# Visualize the matches\n",
    "match_img = matcher.visualize_matches(\n",
    "    img_left, kp_left,\n",
    "    img_right, kp_right,\n",
    "    stereo_matches,\n",
    "    max_matches=50\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(match_img)\n",
    "plt.title('Stereo Feature Matches (Left â†” Right)', fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… These matched features will be used to calculate 3D depth.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Stereo Visual Odometry\n",
    "\n",
    "Now we'll run our complete stereo VO pipeline on a sequence of frames. The system will:\n",
    "1. For each frame pair, detect features in both left and right images\n",
    "2. Match left-right to calculate 3D positions via triangulation\n",
    "3. Track features between consecutive left images (time `t` â†’ `t+1`)\n",
    "4. Use PnP (Perspective-n-Point) to estimate camera motion from 3D-2D correspondences\n",
    "5. Accumulate the motion to build the full trajectory\n",
    "\n",
    "**Key difference from Chapter 1**: We now have **real-world scale** without needing to cheat with ground truth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run Stereo Visual Odometry ---\n",
    "NUM_FRAMES = 200\n",
    "\n",
    "print(f\"Running stereo visual odometry on {NUM_FRAMES} frames...\\n\")\n",
    "\n",
    "stereo_estimated_poses = run_stereo_vo_pipeline(\n",
    "    camera_matrix_left=K_left,\n",
    "    camera_matrix_right=K_right,\n",
    "    baseline=baseline,\n",
    "    left_image_paths=left_image_paths,\n",
    "    right_image_paths=right_image_paths,\n",
    "    ground_truth_poses=ground_truth_poses,\n",
    "    num_frames=NUM_FRAMES,\n",
    "    detector_type='ORB',\n",
    "    max_features=2000\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Stereo VO complete! Estimated {len(stereo_estimated_poses)} poses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compare with Monocular VO (Chapter 1)\n",
    "\n",
    "To really appreciate what we've gained, let's run the monocular VO system from Chapter 1 on the same sequence and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run Monocular VO for comparison ---\n",
    "print(f\"Running monocular visual odometry (Chapter 1) for comparison...\\n\")\n",
    "\n",
    "mono_estimated_poses = run_vo_pipeline(\n",
    "    camera_matrix=K_left,\n",
    "    image_paths=left_image_paths,\n",
    "    ground_truth_poses=ground_truth_poses,\n",
    "    num_frames=NUM_FRAMES,\n",
    "    detector_type='ORB',\n",
    "    max_features=2000\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Monocular VO complete! Estimated {len(mono_estimated_poses)} poses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize and Compare Results\n",
    "\n",
    "Now let's plot all three trajectories: ground truth, monocular VO, and stereo VO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Top-Down Comparison ---\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "\n",
    "# Ground truth\n",
    "gt_pos = ground_truth_poses[:NUM_FRAMES, :3, 3]\n",
    "ax.plot(gt_pos[:, 0], gt_pos[:, 2], 'k--', label='Ground Truth', linewidth=2, alpha=0.7)\n",
    "\n",
    "# Monocular VO (from Chapter 1)\n",
    "mono_pos = mono_estimated_poses[:, :3, 3]\n",
    "ax.plot(mono_pos[:, 0], mono_pos[:, 2], 'r-', label='Monocular VO (Chapter 1)', linewidth=2, alpha=0.7)\n",
    "\n",
    "# Stereo VO (Chapter 2)\n",
    "stereo_pos = stereo_estimated_poses[:, :3, 3]\n",
    "ax.plot(stereo_pos[:, 0], stereo_pos[:, 2], 'b-', label='Stereo VO (Chapter 2)', linewidth=2)\n",
    "\n",
    "# Mark start and end\n",
    "ax.scatter(gt_pos[0, 0], gt_pos[0, 2], c='green', s=150, label='Start', zorder=5, edgecolors='black')\n",
    "ax.scatter(gt_pos[-1, 0], gt_pos[-1, 2], c='red', s=150, label='End', zorder=5, edgecolors='black')\n",
    "\n",
    "ax.set_xlabel('X (m)', fontsize=12)\n",
    "ax.set_ylabel('Z (m) - Forward', fontsize=12)\n",
    "ax.set_title('Trajectory Comparison: Monocular vs Stereo Visual Odometry', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ¯ Key Observation:\")\n",
    "print(\"   The stereo VO trajectory (blue) has the CORRECT SCALE without any ground truth reference!\")\n",
    "print(\"   The monocular VO (red) had to be scaled using ground truth data to match reality.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Quantitative Error Analysis\n",
    "\n",
    "Let's calculate the error metrics for both systems to see how much stereo vision improved our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute Errors ---\n",
    "from src.utils import compute_trajectory_error\n",
    "\n",
    "# Monocular VO error\n",
    "mono_trans_errors, mono_rot_errors = compute_trajectory_error(\n",
    "    mono_estimated_poses,\n",
    "    ground_truth_poses[:len(mono_estimated_poses)]\n",
    ")\n",
    "\n",
    "# Stereo VO error\n",
    "stereo_trans_errors, stereo_rot_errors = compute_trajectory_error(\n",
    "    stereo_estimated_poses,\n",
    "    ground_truth_poses[:len(stereo_estimated_poses)]\n",
    ")\n",
    "\n",
    "# --- Plot Error Comparison ---\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Translation error\n",
    "axes[0].plot(mono_trans_errors, 'r-', label='Monocular VO', alpha=0.7, linewidth=2)\n",
    "axes[0].plot(stereo_trans_errors, 'b-', label='Stereo VO', alpha=0.7, linewidth=2)\n",
    "axes[0].set_ylabel('Translation Error (m)', fontsize=11)\n",
    "axes[0].set_title('Position Error Over Time', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Rotation error\n",
    "axes[1].plot(mono_rot_errors, 'r-', label='Monocular VO', alpha=0.7, linewidth=2)\n",
    "axes[1].plot(stereo_rot_errors, 'b-', label='Stereo VO', alpha=0.7, linewidth=2)\n",
    "axes[1].set_xlabel('Frame', fontsize=11)\n",
    "axes[1].set_ylabel('Rotation Error (degrees)', fontsize=11)\n",
    "axes[1].set_title('Rotation Error Over Time', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nðŸ“Š Error Summary:\")\n",
    "print(\"\\nMonocular VO (Chapter 1):\")\n",
    "print(f\"  Mean translation error: {np.mean(mono_trans_errors):.3f} m\")\n",
    "print(f\"  RMS translation error:  {np.sqrt(np.mean(mono_trans_errors**2)):.3f} m\")\n",
    "print(f\"  Mean rotation error:    {np.mean(mono_rot_errors):.3f}Â°\")\n",
    "\n",
    "print(\"\\nStereo VO (Chapter 2):\")\n",
    "print(f\"  Mean translation error: {np.mean(stereo_trans_errors):.3f} m\")\n",
    "print(f\"  RMS translation error:  {np.sqrt(np.mean(stereo_trans_errors**2)):.3f} m\")\n",
    "print(f\"  Mean rotation error:    {np.mean(stereo_rot_errors):.3f}Â°\")\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = (np.mean(mono_trans_errors) - np.mean(stereo_trans_errors)) / np.mean(mono_trans_errors) * 100\n",
    "print(f\"\\nâœ¨ Stereo VO reduced translation error by {improvement:.1f}%!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: 3D Trajectory Visualization\n",
    "\n",
    "Finally, let's look at the 3D trajectories to see how both systems perform in recovering the altitude changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3D Visualization ---\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Ground truth\n",
    "ax.plot(gt_pos[:, 0], gt_pos[:, 2], -gt_pos[:, 1],\n",
    "        'k--', label='Ground Truth', linewidth=2, alpha=0.7)\n",
    "\n",
    "# Monocular VO\n",
    "ax.plot(mono_pos[:, 0], mono_pos[:, 2], -mono_pos[:, 1],\n",
    "        'r-', label='Monocular VO', linewidth=2, alpha=0.7)\n",
    "\n",
    "# Stereo VO\n",
    "ax.plot(stereo_pos[:, 0], stereo_pos[:, 2], -stereo_pos[:, 1],\n",
    "        'b-', label='Stereo VO', linewidth=2)\n",
    "\n",
    "# Start marker\n",
    "ax.scatter(gt_pos[0, 0], gt_pos[0, 2], -gt_pos[0, 1],\n",
    "           c='green', s=100, label='Start', zorder=5)\n",
    "\n",
    "ax.set_xlabel('X (m)', fontsize=11)\n",
    "ax.set_ylabel('Z (m) - Forward', fontsize=11)\n",
    "ax.set_zlabel('-Y (m) - Altitude', fontsize=11)\n",
    "ax.set_title('3D Trajectory Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.view_init(elev=20, azim=-60)\n",
    "\n",
    "fig.tight_layout(pad=2.0)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ‰ Stereo VO provides scaled, 3D motion estimation without any external reference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this chapter, we successfully upgraded our visual odometry system by adding a second camera. The key achievements:\n",
    "\n",
    "1. **Scale Recovery**: We can now estimate motion in **real-world meters** without any GPS or ground truth reference.\n",
    "2. **Improved Accuracy**: Stereo VO typically provides more accurate trajectories than monocular VO.\n",
    "3. **3D Reconstruction**: We can calculate the 3D structure of the scene, not just the camera path.\n",
    "\n",
    "**Key Takeaway**: By leveraging the geometry of two cameras with a known baseline, we can transform 2D image observations into 3D world coordinates, solving the fundamental scale ambiguity problem of monocular vision.\n",
    "\n",
    "**What's Next**: In future chapters, we'll explore SLAM (Simultaneous Localization and Mapping), loop closure detection, and sensor fusion with IMUs to build even more robust navigation systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
